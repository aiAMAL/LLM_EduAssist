training_arguments:
  num_train_epochs: 2
  auto_find_batch_size: True
  learning_rate: 1e-5
  warmup_steps: 400  # 500
  per_device_train_batch_size: 2
  weight_decay: 0.02    # reduce it if underfitting
  logging_steps: 10
  eval_strategy: steps
  eval_steps: 500
  save_steps: 100000
  gradient_accumulation_steps: 8
  load_best_model_at_end: True
  report_to: "none"     # wandb; tensorboard; mlflow   ["tensorboard", "wandb"]
  save_total_limit: 3

lora_parameters:
  lora_r: 32
  lora_alpha: 32
  target_modules: ["q", "v"]
  lora_dropout: 0.05
  bias: "none"
  task_type: "SEQ_2_SEQ_LM"