training_arguments:
  num_train_epochs: 3
  auto_find_batch_size: True
  learning_rate: 5e-4  # 2e-4 3e-4   # 1e-3
  warmup_steps: 200  # 500
  per_device_train_batch_size: 2
  weight_decay: 0.03    # reduce it if underfitting
  logging_steps: 10
  eval_strategy: steps
  eval_steps: 500
  save_steps: 100000
  gradient_accumulation_steps: 4
  load_best_model_at_end: True
  report_to: "none"     # wandb; tensorboard; mlflow   ["tensorboard", "wandb"]
  save_total_limit: 3,
  num_workers: 2

lora_parameters:
  lora_r: 32
  lora_alpha: 32
  target_modules: ["q", "v"]
  lora_dropout: 0.05
  bias: "none"
  task_type: "SEQ_2_SEQ_LM"